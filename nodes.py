import torch
import comfy.model_management
import comfy.utils
import node_helpers
from torch.nn.functional import interpolate


# ==============================
# åŸå§‹å•èŠ‚ç‚¹ï¼ˆåŠŸèƒ½å®Œå…¨ä¿ç•™ï¼‰
# ==============================
class PainterI2V:
    """
    Wan2.2 å›¾ç”Ÿè§†é¢‘å¢å¼ºèŠ‚ç‚¹ - è§£å†³4æ­¥LoRAæ…¢åŠ¨ä½œé—®é¢˜
    ä¸“ä¸ºå•å¸§è¾“å…¥ä¼˜åŒ–ï¼Œæå‡è¿åŠ¨å¹…åº¦ï¼Œä¿æŒç”»é¢äº®åº¦ç¨³å®š
    """
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "positive": ("CONDITIONING",),
                "negative": ("CONDITIONING",),
                "vae": ("VAE",),
                "width": ("INT", {"default": 832, "min": 16, "max": 4096, "step": 16}),
                "height": ("INT", {"default": 480, "min": 16, "max": 4096, "step": 16}),
                "length": ("INT", {"default": 81, "min": 1, "max": 4096, "step": 4}),
                "batch_size": ("INT", {"default": 1, "min": 1, "max": 4096}),
                "motion_amplitude": ("FLOAT", {"default": 1.15, "min": 1.0, "max": 2.0, "step": 0.05}),
            },
            "optional": {
                "clip_vision_output": ("CLIP_VISION_OUTPUT",),
                "start_image": ("IMAGE",),
            }
        }

    RETURN_TYPES = ("CONDITIONING", "CONDITIONING", "LATENT")
    RETURN_NAMES = ("positive", "negative", "latent")
    FUNCTION = "execute"
    CATEGORY = "conditioning/video_models"

    def execute(self, positive, negative, vae, width, height, length, batch_size,
                motion_amplitude=1.15, start_image=None, clip_vision_output=None):
        # 1. ä¸¥æ ¼çš„é›¶latentåˆå§‹åŒ–ï¼ˆ4æ­¥LoRAçš„ç”Ÿå‘½çº¿ï¼‰
        latent = torch.zeros([batch_size, 16, ((length - 1) // 4) + 1, height // 8, width // 8], 
                           device=comfy.model_management.intermediate_device())
        
        if start_image is not None:
            # å•å¸§è¾“å…¥å¤„ç†
            start_image = start_image[:1]
            start_image = comfy.utils.common_upscale(
                start_image.movedim(-1, 1), width, height, "bilinear", "center"
            ).movedim(1, -1)
            
            # åˆ›å»ºåºåˆ—ï¼šé¦–å¸§çœŸå®ï¼Œåç»­0.5ç°
            image = torch.ones((length, height, width, start_image.shape[-1]), 
                             device=start_image.device, dtype=start_image.dtype) * 0.5
            image[0] = start_image[0]
            
            concat_latent_image = vae.encode(image[:, :, :, :3])
            
            # å•å¸§maskï¼šä»…çº¦æŸé¦–å¸§
            mask = torch.ones((1, 1, latent.shape[2], concat_latent_image.shape[-2], 
                             concat_latent_image.shape[-1]), 
                            device=start_image.device, dtype=start_image.dtype)
            mask[:, :, 0] = 0.0
            
            # 2. è¿åŠ¨å¹…åº¦å¢å¼ºï¼ˆäº®åº¦ä¿æŠ¤æ ¸å¿ƒç®—æ³•ï¼‰
            if motion_amplitude > 1.0:
                base_latent = concat_latent_image[:, :, 0:1]      # é¦–å¸§
                gray_latent = concat_latent_image[:, :, 1:]       # ç°å¸§
                
                diff = gray_latent - base_latent
                diff_mean = diff.mean(dim=(1, 3, 4), keepdim=True)
                diff_centered = diff - diff_mean
                scaled_latent = base_latent + diff_centered * motion_amplitude + diff_mean
                
                # Clamp & ç»„åˆ
                scaled_latent = torch.clamp(scaled_latent, -6, 6)
                concat_latent_image = torch.cat([base_latent, scaled_latent], dim=2)
            
            # 3. æ³¨å…¥åˆ°conditioning
            positive = node_helpers.conditioning_set_values(
                positive, {"concat_latent_image": concat_latent_image, "concat_mask": mask}
            )
            negative = node_helpers.conditioning_set_values(
                negative, {"concat_latent_image": concat_latent_image, "concat_mask": mask}
            )

            # 4. å‚è€ƒå¸§å¢å¼º
            ref_latent = vae.encode(start_image[:, :, :, :3])
            positive = node_helpers.conditioning_set_values(positive, {"reference_latents": [ref_latent]}, append=True)
            negative = node_helpers.conditioning_set_values(negative, {"reference_latents": [torch.zeros_like(ref_latent)]}, append=True)

        if clip_vision_output is not None:
            positive = node_helpers.conditioning_set_values(positive, {"clip_vision_output": clip_vision_output})
            negative = node_helpers.conditioning_set_values(negative, {"clip_vision_output": clip_vision_output})

        out_latent = {"samples": latent}
        return (positive, negative, out_latent)


# ==============================
# æ”¹è¿›ç‰ˆèŠ‚ç‚¹ï¼ˆæœ€ç»ˆåç§°ï¼šPainterI2V-up WAN2.2 LatentMotion Enhancer by paicat1ï¼‰
# ==============================
class PainterI2V_up_WAN2_2_LatentMotion_Enhancer_by_paicat1:
    """
    PainterI2V-up WAN2.2 LatentMotion Enhancer by paicat1ï¼ˆåˆ†åŒºåŸŸå¹…åº¦+åŠ¨æ€æ¨¡ç³Š+æ‰©å±•åœºæ™¯å™ªå£°ï¼‰
    ä¸“ä¸ºWAN2.2ä¼˜åŒ–ï¼Œæ½œç©ºé—´è¿åŠ¨å¼ºåŒ–æ ¸å¿ƒï¼Œæ–°å¢æ‰‹åŠ¨åœºæ™¯é€‰æ‹©ï¼Œç»•å¼€promptæå–é—®é¢˜ï¼Œ100%æ¿€æ´»åœºæ™¯é€‚é…
    ä½œè€…ï¼špaicat1ï¼ˆGitHubç”¨æˆ·åï¼‰
    """
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "positive": ("CONDITIONING",),
                "negative": ("CONDITIONING",),
                "vae": ("VAE",),
                "width": ("INT", {"default": 832, "min": 16, "max": 4096, "step": 16}),
                "height": ("INT", {"default": 480, "min": 16, "max": 4096, "step": 16}),
                "length": ("INT", {"default": 81, "min": 1, "max": 4096, "step": 4}),
                "batch_size": ("INT", {"default": 1, "min": 1, "max": 4096}),
                "motion_amplitude": ("FLOAT", {"default": 1.15, "min": 1.0, "max": 2.0, "step": 0.05}),
                # æ–°å¢ï¼šæ‰‹åŠ¨åœºæ™¯é€‰æ‹©
                "manual_scene": ("COMBO", {
                    "default": "è‡ªåŠ¨åŒ¹é…",
                    "options": ["è‡ªåŠ¨åŒ¹é…", "åŠ¨æ€ä¸»ä½“ï¼ˆäººç‰©/åŠ¨ç‰©ï¼‰", "è‡ªç„¶ç»†èŠ‚ï¼ˆé£æ™¯/çº¹ç†ï¼‰"]
                }),
                "noise_target": ("COMBO", {
                    "default": "å…¨å±€ï¼ˆå«ç¯å¢ƒï¼‰",
                    "options": ["å…¨å±€ï¼ˆå«ç¯å¢ƒï¼‰", "ä»…åŠ¨ä½œåŠ¨æ€", "ä»…ç¯å¢ƒç»†èŠ‚"]
                }),
                "noise_strength": ("FLOAT", {"default": 0.0, "min": 0.0, "max": 0.5, "step": 0.01}),
                "noise_decay_rate": ("FLOAT", {"default": 0.8, "min": 0.5, "max": 1.0, "step": 0.05}),
                "motion_threshold": ("FLOAT", {"default": 0.3, "min": 0.1, "max": 1.0, "step": 0.05}),
                "action_amplitude_boost": ("FLOAT", {"default": 1.0, "min": 1.0, "max": 2.0, "step": 0.1}),
                "time_smooth_strength": ("FLOAT", {"default": 0.0, "min": 0.0, "max": 0.5, "step": 0.05}),
                "motion_blur_strength": ("FLOAT", {"default": 0.0, "min": 0.0, "max": 0.3, "step": 0.05}),
            },
            "optional": {
                "clip_vision_output": ("CLIP_VISION_OUTPUT",),
                "start_image": ("IMAGE",),
                "motion_mask": ("MASK",),
                "keyframe_image": ("IMAGE",),
                "keyframe_frame_idx": ("INT", {"default": 40, "min": 10, "max": 4096, "step": 1}),
            }
        }

    RETURN_TYPES = ("CONDITIONING", "CONDITIONING", "LATENT", "LATENT")
    RETURN_NAMES = ("positive", "negative", "samples", "samples_2x_upscale")
    FUNCTION = "execute"
    CATEGORY = "Wan Video/Experimental"

    def execute(self, positive, negative, vae, width, height, length, batch_size,
                motion_amplitude=1.15, manual_scene="è‡ªåŠ¨åŒ¹é…", noise_target="å…¨å±€ï¼ˆå«ç¯å¢ƒï¼‰", 
                noise_strength=0.0, noise_decay_rate=0.8, motion_threshold=0.3, 
                action_amplitude_boost=1.0, time_smooth_strength=0.0, motion_blur_strength=0.0, 
                clip_vision_output=None, start_image=None, motion_mask=None, 
                keyframe_image=None, keyframe_frame_idx=40):
        
        if start_image is None:
            raise ValueError("ã€PainterI2V-up WAN2.2 LatentMotion Enhancer by paicat1ã€‘å¿…é¡»æä¾› start_image è¾“å…¥")
        
        # æ„å»ºåŸºç¡€latent
        latent_t = ((length - 1) // 4) + 1
        latent_h, latent_w = height // 8, width // 8
        latent = torch.zeros(
            [batch_size, 16, latent_t, latent_h, latent_w],
            device=comfy.model_management.intermediate_device()
        )
        
        # å¤„ç†start_imageï¼ˆç¡®ä¿3é€šé“ï¼‰
        start_image = start_image[:1].squeeze(0)
        print(f"[é€šé“è°ƒè¯•] åˆå§‹å½¢çŠ¶: {start_image.shape}")
        if start_image.shape[-1] != 3:
            if start_image.shape[-1] == 1:
                start_image = start_image.repeat(1, 1, 3)
            else:
                start_image = start_image[..., :3]
        
        # æ„å»ºå¸§åºåˆ—
        image_seq = torch.ones((length, height, width, 3), device=start_image.device, dtype=start_image.dtype) * 0.5
        image_seq[0] = start_image
        
        # VAEç¼–ç 
        concat_latent_image = vae.encode(image_seq.unsqueeze(0))  # [1, C, T, H, W]
        print(f"[PainterI2V-up WAN2.2 LatentMotion Enhancer by paicat1] ğŸ“¦ æ½œåœ¨ç©ºé—´å½¢çŠ¶: {concat_latent_image.shape}")
        
        # æ„å»ºmask
        mask = torch.ones((1, 1, latent_t, latent_h, latent_w), device=start_image.device, dtype=start_image.dtype)
        mask[:, :, 0] = 0.0
        
        # å¯¹é½æ—¶é—´ç»´åº¦
        if concat_latent_image.shape[2] < latent_t:
            pad_length = latent_t - concat_latent_image.shape[2]
            concat_latent_image = torch.cat([
                concat_latent_image,
                concat_latent_image[:, :, -1:, :, :].repeat(1, 1, pad_length, 1, 1)
            ], dim=2)
            print(f"[ç»´åº¦å¯¹é½] è°ƒæ•´concat_latent_imageæ—¶é—´ç»´åº¦è‡³: {concat_latent_image.shape[2]}ï¼ˆä¸maskä¸€è‡´ï¼‰")
        elif concat_latent_image.shape[2] > latent_t:
            concat_latent_image = concat_latent_image[:, :, :latent_t]

        # åˆ¤æ–­æ˜¯å¦å¯ç”¨é«˜çº§åŠŸèƒ½
        enable_motion_enhance = motion_amplitude > 1.0
        enable_region_boost = enable_motion_enhance and (action_amplitude_boost > 1.0 or motion_mask is not None)
        enable_noise = noise_strength > 0.01
        enable_time_smooth = time_smooth_strength > 0.01
        enable_motion_blur = motion_blur_strength > 0.01

        spatial_motion_mask = None

        # è¿åŠ¨æ©ç ç”Ÿæˆ
        if enable_region_boost or enable_noise or enable_motion_blur:
            if motion_mask is not None:
                # è¡¥å…¨é€šé“ç»´åº¦
                if len(motion_mask.shape) == 3:
                    motion_mask = motion_mask.unsqueeze(1)
                elif len(motion_mask.shape) == 2:
                    motion_mask = motion_mask.unsqueeze(0).unsqueeze(0)
                
                # ç¼©æ”¾åˆ°latentå°ºå¯¸
                spatial_motion_mask = interpolate(
                    motion_mask, 
                    size=(latent_h, latent_w),
                    mode='nearest'
                )
                
                # æ‰©å±•åˆ°æ—¶é—´ç»´åº¦
                spatial_motion_mask = spatial_motion_mask.repeat(1, 1, latent_t, 1, 1)
                print(f"[PainterI2V-up WAN2.2 LatentMotion Enhancer by paicat1] ğŸ–Œï¸ è‡ªåŠ¨å¤„ç†åé®ç½©å½¢çŠ¶: {spatial_motion_mask.shape}")
            else:
                base_latent = concat_latent_image[:, :, 0:1]
                gray_latent = concat_latent_image[:, :, 1:]
                diff = torch.abs(gray_latent - base_latent.mean(dim=2, keepdim=True))
                motion_intensity = diff.mean(dim=1, keepdim=True)
                adaptive_threshold = max(0.1, min(motion_threshold, 1.0))
                smoothed_intensity = interpolate(motion_intensity, size=(latent_t, latent_h, latent_w), mode='trilinear')
                spatial_motion_mask = (smoothed_intensity > adaptive_threshold).float()
                spatial_motion_mask[:, :, 0] = 0.0
                print(f"[PainterI2V-up WAN2.2 LatentMotion Enhancer by paicat1] ğŸ¯ æ©ç é˜ˆå€¼: {adaptive_threshold} | è¦†ç›–ç‡: {spatial_motion_mask.mean().item()*100:.1f}%")

        # è¿åŠ¨å¹…åº¦å¢å¼º
        if enable_motion_enhance:
            base_latent = concat_latent_image[:, :, 0:1]
            gray_latent = concat_latent_image[:, :, 1:]
            diff = gray_latent - base_latent
            diff_mean = diff.mean(dim=(1, 3, 4), keepdim=True)
            diff_centered = diff - diff_mean

            # åˆ†åŒºåŸŸå¢å¼º
            if enable_region_boost and spatial_motion_mask is not None:
                action_mask = spatial_motion_mask[:, :, 1:1+gray_latent.shape[2]]
                diff_centered = diff_centered * (1.0 + (action_amplitude_boost - 1.0) * action_mask)
                print(f"[PainterI2V-up WAN2.2 LatentMotion Enhancer by paicat1] ğŸš€ åˆ†åŒºåŸŸè¿åŠ¨å¢å¼ºç”Ÿæ•ˆ | å¢å¼ºç³»æ•°: {action_amplitude_boost}")

            # åº”ç”¨å¹…åº¦å¢å¼º
            scaled_latent = base_latent + diff_centered * motion_amplitude + diff_mean
            scaled_latent = torch.clamp(scaled_latent, -6, 6)
            concat_latent_image = torch.cat([base_latent, scaled_latent], dim=2)
            print(f"[PainterI2V-up WAN2.2 LatentMotion Enhancer by paicat1] ğŸ“ˆ è¿åŠ¨å¹…åº¦å¢å¼ºå®Œæˆ | å¹…åº¦: {motion_amplitude}")

            # æ—¶é—´å¹³æ»‘
            if enable_time_smooth:
                for t in range(1, concat_latent_image.shape[2]):
                    concat_latent_image[:, :, t] = (
                        concat_latent_image[:, :, t-1] * time_smooth_strength +
                        concat_latent_image[:, :, t] * (1 - time_smooth_strength)
                    )
                print(f"[PainterI2V-up WAN2.2 LatentMotion Enhancer by paicat1] âš¡ æ—¶é—´å¹³æ»‘ç”Ÿæ•ˆ | å¼ºåº¦: {time_smooth_strength}")

        # å™ªå£°æ³¨å…¥
        if enable_noise:
            noise_base = torch.randn_like(concat_latent_image)
            low_freq = interpolate(noise_base, scale_factor=(1.0, 0.5, 0.5), mode='trilinear', align_corners=False)
            low_freq = interpolate(low_freq, size=noise_base.shape[2:], mode='trilinear', align_corners=False)
            high_freq = noise_base - low_freq

            # åœºæ™¯å…³é”®è¯åº“
            SCENE_CATEGORIES = [
                {
                    "name": "dynamic_subject",
                    "keywords": [
                        "person", "people", "human", "man", "woman", "girl", "boy", "child", "face", "portrait",
                        "character", "creature", "animal", "dog", "cat", "bird", "monster", "robot", "figure", "body",
                        "äººç‰©", "äººåƒ", "è§’è‰²", "ç”Ÿç‰©", "åŠ¨ç‰©", "äººè„¸", "è‚–åƒ", "å¥³å­©", "ç”·å­©", "å°å­©", "æœºå™¨äºº", "èº«ä½“", "æ¨¡ç‰¹"
                    ],
                    "low_ratio": 0.7,
                    "high_ratio": 0.3,
                },
                {
                    "name": "natural_detail",
                    "keywords": [
                        "nature", "forest", "tree", "leaf", "water", "fire", "smoke", "cloud", "sky", "mountain",
                        "particle", "dust", "spark", "rain", "snow", "fog", "mist", "light ray", "bokeh", "texture",
                        "grass", "flower", "ocean", "river", "storm", "explosion", "magic", "aura", "fluid", "wave",
                        "è‡ªç„¶", "æ£®æ—", "æ ‘å¶", "æ°´", "ç«", "çƒŸ", "äº‘", "å¤©ç©º", "å±±è„‰", "ç²’å­", "ç°å°˜", "ç«èŠ±",
                        "é›¨", "é›ª", "é›¾", "å…‰æ•ˆ", "æ™¯æ·±", "çº¹ç†", "è‰åœ°", "èŠ±æœµ", "æµ·æ´‹", "æ²³æµ", "é£æš´", "çˆ†ç‚¸", "é­”æ³•", "æµä½“", "æ³¢æµª"
                    ],
                    "low_ratio": 0.3,
                    "high_ratio": 0.7,
                }
            ]

            # æ‰‹åŠ¨åœºæ™¯ä¼˜å…ˆé€‚é…
            dynamic_noise = None
            if manual_scene == "åŠ¨æ€ä¸»ä½“ï¼ˆäººç‰©/åŠ¨ç‰©ï¼‰":
                matched_category = SCENE_CATEGORIES[0]
                dynamic_noise = low_freq * matched_category["low_ratio"] + high_freq * matched_category["high_ratio"]
                print(f"[å™ªå£°é€‚é…] æ‰‹åŠ¨é€‰æ‹©åœºæ™¯: {matched_category['name']}ç±» | ä½é¢‘å æ¯”: {matched_category['low_ratio']}")
            elif manual_scene == "è‡ªç„¶ç»†èŠ‚ï¼ˆé£æ™¯/çº¹ç†ï¼‰":
                matched_category = SCENE_CATEGORIES[1]
                dynamic_noise = low_freq * matched_category["low_ratio"] + high_freq * matched_category["high_ratio"]
                print(f"[å™ªå£°é€‚é…] æ‰‹åŠ¨é€‰æ‹©åœºæ™¯: {matched_category['name']}ç±» | ä½é¢‘å æ¯”: {matched_category['low_ratio']}")
            else:
                # è‡ªåŠ¨åŒ¹é…é€»è¾‘
                prompt_text = ""
                for cond in positive:
                    if isinstance(cond, (list, tuple)) and len(cond) > 1:
                        meta = cond[1]
                        if isinstance(meta, dict) and "text" in meta:
                            prompt_text += meta["text"].lower() + " "
                print(f"[è°ƒè¯•] è‡ªåŠ¨åŒ¹é…æ¨¡å¼ - æå–åˆ°çš„prompt: {prompt_text}")

                if not prompt_text.strip():
                    dynamic_noise = (low_freq + high_freq) * 0.5
                    print(f"[å™ªå£°é€‚é…] è‡ªåŠ¨åŒ¹é… - æœªåŒ¹é…ç‰¹å®šåœºæ™¯ï¼Œä½¿ç”¨é»˜è®¤æ¯”ä¾‹")
                else:
                    matched_category = None
                    for category in SCENE_CATEGORIES:
                        if any(kw in prompt_text for kw in category["keywords"]):
                            matched_category = category
                            break
                    if matched_category:
                        dynamic_noise = low_freq * matched_category["low_ratio"] + high_freq * matched_category["high_ratio"]
                        print(f"[å™ªå£°é€‚é…] è‡ªåŠ¨åŒ¹é… - åŒ¹é…åœºæ™¯: {matched_category['name']}ç±» | ä½é¢‘å æ¯”: {matched_category['low_ratio']}")
                    else:
                        dynamic_noise = (low_freq + high_freq) * 0.5
                        print(f"[å™ªå£°é€‚é…] è‡ªåŠ¨åŒ¹é… - æœªåŒ¹é…ç‰¹å®šåœºæ™¯ï¼Œä½¿ç”¨é»˜è®¤æ¯”ä¾‹")

            # åº”ç”¨å™ªå£°ç›®æ ‡åŒºåŸŸ
            if noise_target == "ä»…åŠ¨ä½œåŠ¨æ€" and spatial_motion_mask is not None:
                if spatial_motion_mask.shape[2] != dynamic_noise.shape[2]:
                    spatial_motion_mask = interpolate(spatial_motion_mask, size=(dynamic_noise.shape[2], latent_h, latent_w), mode='nearest')
                dynamic_noise = dynamic_noise * spatial_motion_mask
            elif noise_target == "ä»…ç¯å¢ƒç»†èŠ‚" and spatial_motion_mask is not None:
                if spatial_motion_mask.shape[2] != dynamic_noise.shape[2]:
                    spatial_motion_mask = interpolate(spatial_motion_mask, size=(dynamic_noise.shape[2], latent_h, latent_w), mode='nearest')
                dynamic_noise = dynamic_noise * (1.0 - spatial_motion_mask)

            # æ—¶é—´è¡°å‡
            time_weights = torch.linspace(1.0, noise_decay_rate, dynamic_noise.shape[2], device=dynamic_noise.device)
            dynamic_noise = dynamic_noise * time_weights.view(1, 1, -1, 1, 1)
            
            # æ³¨å…¥å¹¶è£å‰ª
            concat_latent_image = concat_latent_image + dynamic_noise * noise_strength
            concat_latent_image = torch.clamp(concat_latent_image, -6, 6)
            print(f"[PainterI2V-up WAN2.2 LatentMotion Enhancer by paicat1] âœ… å™ªå£°æ³¨å…¥ç”Ÿæ•ˆ | ç›®æ ‡: {noise_target} | å¼ºåº¦: {noise_strength}")

        # åŠ¨æ€æ¨¡ç³Š
        if enable_motion_blur and spatial_motion_mask is not None:
            blurred = concat_latent_image.clone()
            for t in range(1, concat_latent_image.shape[2]):
                region = spatial_motion_mask[:, :, t] > 0.5
                if region.any():
                    blurred[:, :, t, region.squeeze(0).squeeze(0)] = (
                        concat_latent_image[:, :, t-1, region.squeeze(0).squeeze(0)] * motion_blur_strength +
                        concat_latent_image[:, :, t, region.squeeze(0).squeeze(0)] * (1 - motion_blur_strength)
                    )
            concat_latent_image = blurred
            print(f"[PainterI2V-up WAN2.2 LatentMotion Enhancer by paicat1] ğŸŒªï¸ åŠ¨æ€æ¨¡ç³Šç”Ÿæ•ˆ | å¼ºåº¦: {motion_blur_strength}")

        # è®¾ç½®Conditioning
        positive = node_helpers.conditioning_set_values(positive, {"concat_latent_image": concat_latent_image, "concat_mask": mask})
        negative = node_helpers.conditioning_set_values(negative, {"concat_latent_image": concat_latent_image, "concat_mask": mask})

        print(f"[é€šé“è°ƒè¯•] ç¼–ç refå‰å½¢çŠ¶: {start_image.unsqueeze(0).shape}")
        ref_latent = vae.encode(start_image.unsqueeze(0)[:, :, :, :3])
        positive = node_helpers.conditioning_set_values(positive, {"reference_latents": [ref_latent]}, append=True)
        negative = node_helpers.conditioning_set_values(negative, {"reference_latents": [torch.zeros_like(ref_latent)]}, append=True)

        if clip_vision_output is not None:
            positive = node_helpers.conditioning_set_values(positive, {"clip_vision_output": clip_vision_output})
            negative = node_helpers.conditioning_set_values(negative, {"clip_vision_output": clip_vision_output})

        # è¾“å‡ºæ„å»º
        samples = {"samples": latent, "batch_size": batch_size, "frame_rate": 24}
        B, C, T, H, W = latent.shape
        latent_4d = latent.reshape(B * T, C, H, W)
        upscaled_4d = interpolate(latent_4d, size=(H*2, W*2), mode='bilinear', align_corners=False)
        upscaled_latent = upscaled_4d.reshape(B, C, T, H*2, W*2)
        samples_2x_upscale = {"samples": upscaled_latent, "batch_size": batch_size, "frame_rate": 24}
        print(f"[PainterI2V-up WAN2.2 LatentMotion Enhancer by paicat1] ğŸ“¤ è¾“å‡ºå®Œæˆ | åŸå°ºå¯¸: {latent.shape} | 2å€æ”¾å¤§: {upscaled_latent.shape}")

        return (positive, negative, samples, samples_2x_upscale)


# ==============================
# èŠ‚ç‚¹æ³¨å†Œï¼ˆç¡®ä¿ComfyUIæ­£å¸¸è¯†åˆ«ï¼‰
# ==============================
NODE_CLASS_MAPPINGS = {
    "PainterI2V": PainterI2V,
    "PainterI2V-up WAN2.2 LatentMotion Enhancer by paicat1": PainterI2V_up_WAN2_2_LatentMotion_Enhancer_by_paicat1
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "PainterI2V": "ğŸ¨ PainterI2V (Wan2.2 æ…¢åŠ¨ä½œä¿®å¤)",
    "PainterI2V-up WAN2.2 LatentMotion Enhancer by paicat1": "ğŸš€ PainterI2V-up WAN2.2 LatentMotion Enhancer by paicat1ï¼ˆæ½œç©ºé—´è¿åŠ¨å¼ºåŒ–ï¼‰"
}
